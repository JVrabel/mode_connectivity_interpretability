{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/data_setup.py\n",
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoaders for \n",
    "LIBS benchmark classification dataset.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from load_libs_data import load_contest_train_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from utils import resample_spectra_df\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    batch_size: int, \n",
    "    num_classes: int,\n",
    "    device: torch.device,\n",
    "    num_workers: int=NUM_WORKERS, \n",
    "    split_rate: float=0.5,\n",
    "    random_st: int=102,\n",
    "    spectra_count: int=50\n",
    "):\n",
    "    \"\"\"Creates training and validation DataLoaders.\n",
    "    ...\n",
    "    \"\"\"\n",
    "    \n",
    "    pickle_file_path = \"data/data.pkl\"\n",
    "    \n",
    "    if os.path.exists(pickle_file_path):\n",
    "        with open(pickle_file_path, 'rb') as f:\n",
    "            data_dict = pickle.load(f)\n",
    "        X = data_dict['X']\n",
    "        y = data_dict['y']\n",
    "        samples = data_dict['samples']\n",
    "    else:\n",
    "        X, y, samples = load_contest_train_dataset(train_dir, spectra_count)\n",
    "        with open(pickle_file_path, 'wb') as f:\n",
    "            pickle.dump({'X': X, 'y': y, 'samples': samples}, f)\n",
    "        \n",
    "    wavelengths = X.columns\n",
    "    \n",
    "    new_wave = np.arange(400, 600, 0.08)\n",
    "    X_new = resample_spectra_df(X, wavelengths, new_wave)\n",
    "    del X\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_new, y, test_size=split_rate, random_state=random_st, stratify=samples, shuffle=True)\n",
    "    del y, samples, X_new\n",
    "    \n",
    "    y_train = y_train-1\n",
    "    y_val = y_val-1\n",
    "    \n",
    "    scaler = Normalizer(norm='max')\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "    X_val = torch.from_numpy(X_val).float()\n",
    "    \n",
    "    y_train = torch.from_numpy(np.array(y_train)).long()\n",
    "    y_val = torch.from_numpy(np.array(y_val)).long()\n",
    "    \n",
    "    X_train = X_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataloader, test_dataloader, y_train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "new_wave = np.arange(400, 600, 0.08)\n",
    "new_wave.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/model_builder.py\n",
    "\"\"\"\n",
    "Contains PyTorch model code to instantiate an MLP model.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"Creates a simple MLP architecture.\n",
    "\n",
    "    Args:\n",
    "        input_shape: An integer indicating the size of the input vector.\n",
    "        hidden_units1: An integer indicating the number of hidden units in the first hidden layer.\n",
    "        hidden_units2: An integer indicating the number of hidden units in the second hidden layer.\n",
    "        output_shape: An integer indicating the number of output units.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units1: int, hidden_units2: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # First hidden layer\n",
    "        self.hidden_layer_1 = nn.Sequential(\n",
    "            nn.Linear(input_shape, hidden_units1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Second hidden layer\n",
    "        self.hidden_layer_2 = nn.Sequential(\n",
    "            nn.Linear(hidden_units1, hidden_units2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_units2, output_shape)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/engine.py\n",
    "\"\"\"\n",
    "Contains functions for training and testing a PyTorch model.\n",
    "\"\"\"\n",
    "import torch\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from loss_penalization import sparseloc\n",
    "import time\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device,\n",
    "               regularization_type: str,\n",
    "               reg_lambda: int) -> Tuple[float, float]:\n",
    "  \"\"\"Trains a PyTorch model for a single epoch.\n",
    "\n",
    "  Turns a target PyTorch model to training mode and then\n",
    "  runs through all of the required training steps (forward\n",
    "  pass, loss calculation, optimizer step).\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be trained.\n",
    "    dataloader: A DataLoader instance for the model to be trained on.\n",
    "    loss_fn: A PyTorch loss function to minimize.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A tuple of training loss and training accuracy metrics.\n",
    "    In the form (train_loss, train_accuracy). For example:\n",
    "\n",
    "    (0.1112, 0.8743)\n",
    "  \"\"\"\n",
    "  # Put model in train mode\n",
    "  model.train()\n",
    "\n",
    "  # Setup train loss and train accuracy values\n",
    "  train_loss, train_acc = 0, 0\n",
    "\n",
    "  # Loop through data loader data batches\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "      # Send data to target device\n",
    "      X, y = X.to(device), y.to(device)\n",
    "\n",
    "      # 1. Forward pass\n",
    "      y_pred = model(X)\n",
    "      # print(y_pred.shape)\n",
    "      # print(y.shape)\n",
    "      # 2. Calculate  and accumulate loss\n",
    "\n",
    "      if regularization_type == \"vanilla\":\n",
    "          # No regularization\n",
    "          reg_term = 0 \n",
    "      elif regularization_type == \"L1\":\n",
    "          # Apply L1 regularization\n",
    "          reg_term = torch.sum( torch.abs(model.hidden_layer_1[0].weight))\n",
    "      elif regularization_type == \"sparseloc\":\n",
    "          # Apply Sparse Localized regularization\n",
    "          reg_term = sparseloc(model.hidden_layer_1[0].weight)\n",
    "      else:\n",
    "          raise ValueError(\"Invalid regularization type. Options are 'vanilla', 'L1', 'sparseloc'.\")\n",
    "           \n",
    "\n",
    "      # loss\n",
    "      loss = loss_fn(y_pred, y) + reg_lambda * reg_term\n",
    "      train_loss += loss.item() \n",
    "\n",
    "      # 3. Optimizer zero grad\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # 4. Loss backward\n",
    "      loss.backward()\n",
    "\n",
    "      # 5. Optimizer step\n",
    "      optimizer.step()\n",
    "\n",
    "      # Calculate and accumulate accuracy metric across all batches\n",
    "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "  # Adjust metrics to get average loss and accuracy per batch \n",
    "  train_loss = train_loss / len(dataloader)\n",
    "  train_acc = train_acc / len(dataloader)\n",
    "  return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "  \"\"\"Tests a PyTorch model for a single epoch.\n",
    "\n",
    "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "  a forward pass on a testing dataset.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "    (0.0223, 0.8985)\n",
    "  \"\"\"\n",
    "  # Put model in eval mode\n",
    "  model.eval() \n",
    "\n",
    "  # Setup test loss and test accuracy values\n",
    "  test_loss, test_acc = 0, 0\n",
    "\n",
    "  # Turn on inference context manager\n",
    "  with torch.no_grad():\n",
    "      # Loop through DataLoader batches\n",
    "      for batch, (X, y) in enumerate(dataloader):\n",
    "          # Send data to target device\n",
    "          X, y = X.to(device), y.to(device)\n",
    "\n",
    "          # 1. Forward pass\n",
    "          test_pred_logits = model(X)\n",
    "          # print(test_pred_logits.shape)\n",
    "\n",
    "          # 2. Calculate and accumulate loss\n",
    "          loss = loss_fn(test_pred_logits, y)\n",
    "          test_loss += loss.item()\n",
    "\n",
    "          # Calculate and accumulate accuracy\n",
    "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "  # Adjust metrics to get average loss and accuracy per batch \n",
    "  test_loss = test_loss / len(dataloader)\n",
    "  test_acc = test_acc / len(dataloader)\n",
    "  return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device,\n",
    "          reg_lambda: float,\n",
    "          regularization_type: str,\n",
    "          trial_num: int,\n",
    "          project_wandb: str,\n",
    "          entity_wandb: str) -> Dict[str, List]:\n",
    "  \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "  Passes a target PyTorch models through train_step() and test_step()\n",
    "  functions for a number of epochs, training and testing the model\n",
    "  in the same epoch loop.\n",
    "\n",
    "  Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "  Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for \n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "                  train_acc: [...],\n",
    "                  test_loss: [...],\n",
    "                  test_acc: [...]} \n",
    "    For example if training for epochs=2: \n",
    "                 {train_loss: [2.0616, 1.0537],\n",
    "                  train_acc: [0.3945, 0.3945],\n",
    "                  test_loss: [1.2641, 1.5706],\n",
    "                  test_acc: [0.3400, 0.2973]} \n",
    "  \"\"\"\n",
    "  # Create empty results dictionary\n",
    "  results = {\"train_loss\": [],\n",
    "      \"train_acc\": [],\n",
    "      \"test_loss\": [],\n",
    "      \"test_acc\": []\n",
    "  }\n",
    "\n",
    "  run_name = f\"{regularization_type}_lambda_{reg_lambda}_trial_{trial_num}\"  # <-- Modified\n",
    "  \n",
    "  wandb.init(project=project_wandb, entity=entity_wandb, name=run_name, tags=[f\"{regularization_type}\", f\"lambda_{reg_lambda}\", f\"trial_{trial_num}\"])\n",
    "  wandb.watch(model)  \n",
    "  # Loop through training and testing steps for a number of epochs\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "      train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device,\n",
    "                                          regularization_type = regularization_type,\n",
    "                                          reg_lambda = reg_lambda)\n",
    "      test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "      # Print out what's happening\n",
    "      print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "      )\n",
    "      wandb.log({\n",
    "        \"epoch\": epoch+1,\n",
    "        \"lambda\": reg_lambda,\n",
    "        \"training_loss\": train_loss, \n",
    "        \"train_acc\": train_acc,\n",
    "        \"validation_loss\": test_loss,\n",
    "        \"validation_acc\": test_acc\n",
    "      })  \n",
    "\n",
    "      # Update results dictionary\n",
    "      results[\"train_loss\"].append(train_loss)\n",
    "      results[\"train_acc\"].append(train_acc)\n",
    "      results[\"test_loss\"].append(test_loss)\n",
    "      results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "  wandb.finish()  \n",
    "  # Return the filled results at the end of the epochs\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/run_experiment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/run_experiment.py\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# List of lambda values you want to experiment with\n",
    "lambda_values = [0.005, 0.002, 0.001, 0.0005, 0.0002, 0.0001, 0.00005, 0.00002, 0.00001]\n",
    "\n",
    "# Number of trials for each lambda value\n",
    "num_trials = 5  # Feel free to change this\n",
    "\n",
    "# Dictionary to hold metrics for each lambda value\n",
    "all_metrics = {}\n",
    "\n",
    "# Run the training script with each lambda value\n",
    "for lambda_val in lambda_values:\n",
    "    for trial_num in range(1, num_trials + 1):  # trial_num starts from 1\n",
    "        # Execute the training script, passing in lambda and trial number as command line arguments\n",
    "        subprocess.run(['python', \n",
    "                        'src/train.py', \n",
    "                        '--reg_lambda', str(lambda_val), \n",
    "                        '--reg_type', 'L1', \n",
    "                        '--save_model', 'False', \n",
    "                        '--trial', str(trial_num)\n",
    "                        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0050, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0520, 0.0439, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.1077, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.2728, 0.1715, 0.0000]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_object = torch.rand(1024, 40000)\n",
    "\n",
    "# If you're planning to run these lines in a loop, consider using batch operations for speedup\n",
    "\n",
    "# Extract W1 and initialize W1_out to zeros to save memory.\n",
    "W1 = tensor_object[0,:]\n",
    "W1_out = torch.zeros(W1.size(0), W1.size(0))\n",
    "\n",
    "# Calculate the outer product in-place to save memory.\n",
    "torch.outer(W1, W1, out=W1_out)\n",
    "\n",
    "# Zero out the elements above the diagonal, including the diagonal\n",
    "W1_out.tril_(-1)\n",
    "\n",
    "# Zero out elements further than N diagonals below the diagonal in-place\n",
    "W1_out.triu_(-4)\n",
    "\n",
    "W1_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum: tensor(1047492.8750)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor_object = torch.rand(512, 2048)\n",
    "tensor_object.to(device)\n",
    "\n",
    "total_sum = 0.0  # variable to store the total sum\n",
    "W_out = torch.zeros(tensor_object[0,:].size(0), tensor_object[0,:].size(0))\n",
    "# Loop over the rows of the tensor_object\n",
    "for i in range(tensor_object.size(0)):\n",
    "    W = tensor_object[i, :]\n",
    "\n",
    "    # Compute the outer product\n",
    "    torch.outer(W, W, out=W_out)\n",
    "\n",
    "    # Zero out the elements above the diagonal and far from the diagonal\n",
    "    W_out.tril_(-1)\n",
    "    W_out.triu_(-4)\n",
    "    # print(torch.sum(W_out))\n",
    "    # Sum the values and add to total_sum\n",
    "    total_sum += torch.sum(W_out)\n",
    "\n",
    "print(\"Total sum:\", total_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sparseloc(weight_tensor):\n",
    "    total_sum = 0.0  # variable to store the total sum\n",
    "\n",
    "    # Loop over the rows of the weight_tensor\n",
    "    for i in range(weight_tensor.size(0)):\n",
    "        W = weight_tensor[i, :]\n",
    "\n",
    "        # Compute the outer product directly to make it differentiable\n",
    "        W_out = torch.outer(W, W)\n",
    "\n",
    "        # Zero out the elements above the diagonal and far from the diagonal\n",
    "        lower_triangle = torch.tril(W_out, -1)\n",
    "        band_matrix = torch.triu(lower_triangle, -3)\n",
    "\n",
    "        # Sum the absolute values and add to total_sum\n",
    "        total_sum += torch.sum(torch.abs(band_matrix))\n",
    "\n",
    "    return total_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/loss_penalization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/loss_penalization.py\n",
    "\"\"\"\n",
    "Contains functions for computing a custom term to regularize the loss function. \n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# def sparseloc(weight_tensor):\n",
    "#     total_sum = 0.0  # variable to store the total sum\n",
    "\n",
    "#     # Initialize W_out to zeros to save memory\n",
    "#     W_out = torch.zeros(weight_tensor[0,:].size(0), weight_tensor[0,:].size(0))\n",
    "\n",
    "#     # Loop over the rows of the weight_tensor\n",
    "#     for i in range(weight_tensor.size(0)):\n",
    "#         W = weight_tensor[i, :]\n",
    "\n",
    "#         # Compute the outer product directly to make it differentiable\n",
    "#         W_out = torch.outer(W, W)\n",
    "\n",
    "#         # Zero out the elements above the diagonal and far from the diagonal\n",
    "#         W_out.tril_(-1)\n",
    "#         W_out.triu_(-4)\n",
    "\n",
    "#         # Sum the absolute values and add to total_sum\n",
    "#         total_sum += torch.sum(torch.abs(W_out))\n",
    "\n",
    "#     return total_sum\n",
    "\n",
    "import torch\n",
    "\n",
    "def sparseloc(weight_tensor):\n",
    "    total_sum = 0.0  # variable to store the total sum\n",
    "\n",
    "    # Loop over the rows of the weight_tensor\n",
    "    for i in range(weight_tensor.size(0)):\n",
    "        W = weight_tensor[i, :]\n",
    "\n",
    "        # Compute the outer product directly to make it differentiable\n",
    "        W_out = torch.outer(W, W)\n",
    "\n",
    "        # Zero out the elements above the diagonal and far from the diagonal\n",
    "        lower_triangle = torch.tril(W_out, -1)\n",
    "        band_matrix = torch.triu(lower_triangle, -3)\n",
    "\n",
    "        # Sum the absolute values and add to total_sum\n",
    "        total_sum += torch.sum(torch.abs(band_matrix))\n",
    "\n",
    "    return total_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils.py\n",
    "\"\"\"\n",
    "Contains various utility functions for PyTorch model training and saving.\n",
    "\"\"\"\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import pandas as pd\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "  \"\"\"Saves a PyTorch model to a target directory.\n",
    "\n",
    "  Args:\n",
    "    model: A target PyTorch model to save.\n",
    "    target_dir: A directory for saving the model to.\n",
    "    model_name: A filename for the saved model. Should include\n",
    "      either \".pth\" or \".pt\" as the file extension.\n",
    "\n",
    "  Example usage:\n",
    "    save_model(model=model_0,\n",
    "               target_dir=\"models\",\n",
    "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
    "  \"\"\"\n",
    "  # Create target directory\n",
    "  target_dir_path = Path(target_dir)\n",
    "  target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "\n",
    "  # Create model save path\n",
    "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "  model_save_path = target_dir_path / model_name\n",
    "\n",
    "  # Save the model state_dict()\n",
    "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "  torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resample_spectra_df(df, original_wavelengths, new_wavelengths):\n",
    "    \"\"\"\n",
    "    Resample the spectra in dataset X (as a DataFrame) to new wavelengths using linear interpolation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, where each row is a spectrum.\n",
    "    - original_wavelengths: 1D array of original wavelengths.\n",
    "    - new_wavelengths: 1D array of new wavelengths.\n",
    "\n",
    "    Returns:\n",
    "    - df_resampled: DataFrame of resampled spectra.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert DataFrame to NumPy array\n",
    "    X = df.values\n",
    "\n",
    "    num_spectra = X.shape[0]\n",
    "    X_resampled = np.zeros((num_spectra, len(new_wavelengths)))\n",
    "\n",
    "    for i in range(num_spectra):\n",
    "        f = interp1d(original_wavelengths, X[i, :], kind='linear', fill_value='extrapolate')\n",
    "        X_resampled[i, :] = f(new_wavelengths)\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    df_resampled = pd.DataFrame(X_resampled, columns=new_wavelengths)\n",
    "\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/train.py\n",
    "\"\"\"\n",
    "Trains a PyTorch model using device-agnostic code.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import data_setup, engine, model_builder, utils\n",
    "import win32com.client\n",
    "# Add this at the top of your script\n",
    "import argparse\n",
    "\n",
    "# Argument parser setup\n",
    "parser = argparse.ArgumentParser(description='Training script for PyTorch model.')\n",
    "parser.add_argument('--epochs', type=int, default=300, help='Number of epochs to run')\n",
    "parser.add_argument('--reg_type', type=str, default='vanilla', choices=['vanilla', 'L1', 'L2'], help='Type of regularization to apply')\n",
    "parser.add_argument('--reg_lambda', type=float, default=0.1, help='Regularization strength')\n",
    "parser.add_argument('--save_model', type=bool, default=False, help='Flag to indicate if the model should be saved')\n",
    "parser.add_argument('--trials', type=int, default=1, help='Number of trials for each setup')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Then use args to set your hyperparameters\n",
    "NUM_EPOCHS = args.epochs\n",
    "regularization_type = args.reg_type\n",
    "reg_lambda = args.reg_lambda\n",
    "save_model = args.save_model\n",
    "trials = args.trials\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup hyperparameters\n",
    "# NUM_EPOCHS = 300\n",
    "# regularization_type = \"L1\" # \"vanilla\" \"sparseloc\"\n",
    "# reg_lambda = 0.1\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SHAPE = 2500  # Modify this based on your actual input vector length\n",
    "OUTPUT_SHAPE = 12\n",
    "HIDDEN_UNITS1 = 128  # Number of neurons in the first hidden layer\n",
    "HIDDEN_UNITS2 = 64  # Number of neurons in the second hidden layer\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "PROJECT_WANDB = 'interpretability_mode_connectivity'\n",
    "ENTITY_WANDB = 'jakubv'\n",
    "\n",
    "# Setup directories for data - modify these paths as needed\n",
    "shell = win32com.client.Dispatch(\"WScript.Shell\")\n",
    "shortcut = shell.CreateShortCut('data/contest_TRAIN.h5.lnk')\n",
    "train_dir = shortcut.Targetpath\n",
    "\n",
    "# train_dir = \"data/train\"\n",
    "# test_dir = \"data/test\"   # this should be val, and also used only if there is a specific dataset for valiadation data. \n",
    "\n",
    "# Setup target device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# Create DataLoaders with help from data_setup.py\n",
    "train_dataloader, test_dataloader, classes = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device = device,\n",
    "    num_classes = OUTPUT_SHAPE\n",
    ")\n",
    "\n",
    "# Create model with help from model_builder.py\n",
    "model = model_builder.SimpleMLP(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    hidden_units1=HIDDEN_UNITS1,\n",
    "    hidden_units2=HIDDEN_UNITS2,\n",
    "    output_shape=OUTPUT_SHAPE\n",
    ").to(device)\n",
    "\n",
    "# Set loss and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE)\n",
    "\n",
    "# Start training with help from engine.py\n",
    "engine.train(model=model,\n",
    "             train_dataloader=train_dataloader,\n",
    "             test_dataloader=test_dataloader,\n",
    "             loss_fn=loss_fn,\n",
    "             optimizer=optimizer,\n",
    "             epochs=NUM_EPOCHS,\n",
    "             device=device,\n",
    "             regularization_type = regularization_type,\n",
    "             reg_lambda=reg_lambda,\n",
    "             trial_num=trials,\n",
    "             project_wandb=PROJECT_WANDB,\n",
    "             entity_wandb=ENTITY_WANDB)\n",
    "\n",
    "\n",
    "# Save the model with help from utils.py\n",
    "if save_model:\n",
    "    model_name = f\"{regularization_type}_lambda_{reg_lambda}_model.pth\"\n",
    "    utils.save_model(model=model,\n",
    "                     target_dir=\"models\",\n",
    "                     model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--reg_lambda 0.0005 --reg_type L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  # Python's garbage collection module\n",
    "\n",
    "# Explicitly delete variables\n",
    "# del X_train\n",
    "gc.collect()  # Call the garbage collector\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/test_data.py\n",
    "# \"\"\"\n",
    "# Contains functionality for creating PyTorch DataLoaders for \n",
    "# LIBS benchmark classification dataset.\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from load_libs_data import load_contest_train_dataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "# from siamese_net import prepare_triplets\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# def create_dataloaders(\n",
    "#     train_dir: str, \n",
    "#     #test_dir: str, \n",
    "#     batch_size: int, \n",
    "#     device: torch.device,\n",
    "#     num_workers: int=NUM_WORKERS, \n",
    "#     split_rate: float=0.6,\n",
    "#     random_st: int=102,\n",
    "#     spectra_count: int=100\n",
    "#     ):\n",
    "#     \"\"\"Creates training and validation DataLoaders.\n",
    "\n",
    "#     Takes in a training directory directory path and split the data\n",
    "#     to train/validation. After, it turns them into PyTorch Datasets and \n",
    "#     then into PyTorch DataLoaders.\n",
    "\n",
    "#     Args:\n",
    "#     train_dir: Path to training directory.\n",
    "#     batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "#     num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "#     Returns:\n",
    "#     A tuple of (train_dataloader, test_dataloader, class_labels).\n",
    "#     Example usage:\n",
    "#         train_dataloader, test_dataloader, class_labels, wavelengths = \\\n",
    "#         = create_dataloaders(train_dir=path/to/train_dir,\n",
    "#                                 test_dir=path/to/test_dir,\n",
    "#                                 transform=some_transform,\n",
    "#                                 batch_size=32,\n",
    "#                                 num_workers=4)\n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#     X, y, samples = load_contest_train_dataset(train_dir, spectra_count)\n",
    "#     wavelengths = X.columns\n",
    "\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=split_rate, random_state=random_st, stratify=samples, shuffle = True)\n",
    "#     del X, y, samples\n",
    "\n",
    "#     if True:\n",
    "#       scaler =  Normalizer(norm = 'max')\n",
    "#       X_train = scaler.fit_transform(X_train)\n",
    "#       X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "#     # Convert data to torch tensors\n",
    "#     X_train = torch.from_numpy(X_train).unsqueeze(1).float() # Add extra dimension for channels\n",
    "#     X_val = torch.from_numpy(X_val).unsqueeze(1).float() # Add extra dimension for channels\n",
    "#     y_train = torch.from_numpy(np.array(y_train)).long()\n",
    "#     y_val = torch.from_numpy(np.array(y_val)).long()\n",
    "\n",
    "#     #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     # If available, move data to the GPU\n",
    "#     X_train.to(device)\n",
    "#     X_val.to(device) \n",
    "#     y_train.to(device)\n",
    "#     y_val.to(device)\n",
    "\n",
    "#     # Prepare triplets for the training, validation\n",
    "#     train_triplets = prepare_triplets(X_train, y_train)\n",
    "#     val_triplets = prepare_triplets(X_val, y_val)\n",
    "\n",
    "\n",
    "#     # Create PyTorch DataLoader objects for the training and validation sets\n",
    "#     train_dataloader = DataLoader(train_triplets, batch_size=batch_size, shuffle=True)\n",
    "#     val_dataloader = DataLoader(val_triplets, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "#     return train_dataloader, val_dataloader, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import funcs\n",
    "# import importlib\n",
    "# importlib.reload(funcs)\n",
    "# from funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/prediction_engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/prediction_engine.py\n",
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoaders for \n",
    "LIBS benchmark classification dataset.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from load_libs_data import load_contest_test_dataset, load_contest_train_dataset\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_dataloaders(\n",
    "    test_dir: str, \n",
    "    test_labels_dir: str, \n",
    "    batch_size: int, \n",
    "    device: torch.device,\n",
    "    pred_test: bool,\n",
    "    ):\n",
    "    \"\"\"Creates training and validation DataLoaders.\n",
    "\n",
    "    Takes in a training directory directory path and split the data\n",
    "    to train/validation. After, it turns them into PyTorch Datasets and \n",
    "    then into PyTorch DataLoaders.\n",
    "\n",
    "    Args:\n",
    "    train_dir: Path to training directory.\n",
    "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "    num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "    Returns:\n",
    "    A tuple of (train_dataloader, test_dataloader, class_labels).\n",
    "    Example usage:\n",
    "        train_dataloader, test_dataloader, class_labels, wavelengths = \\\n",
    "        = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                                test_dir=path/to/test_dir,\n",
    "                                transform=some_transform,\n",
    "                                batch_size=32,\n",
    "                                num_workers=4)\n",
    "    \"\"\"\n",
    "\n",
    "    if pred_test:\n",
    "        X_test = load_contest_test_dataset(test_dir)\n",
    "        y_test = np.loadtxt(test_labels_dir, delimiter = ',')\n",
    "    else: # use with caution, only for predicting training embeddings\n",
    "        X_test, y_test, _ = load_contest_train_dataset(test_dir)\n",
    "\n",
    "    if True:\n",
    "      scaler =  Normalizer(norm = 'max')\n",
    "      X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    # Convert data to torch tensors\n",
    "    X_test = torch.from_numpy(X_test).unsqueeze(1).float() # Add extra dimension for channels\n",
    "    y_test = torch.from_numpy(np.array(y_test)).long()\n",
    "\n",
    "\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # If available, move data to the GPU\n",
    "    X_test.to(device)\n",
    "    y_test.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    # Create PyTorch DataLoader objects for the training and validation sets\n",
    "    pred_test_loader = DataLoader(X_test, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    return pred_test_loader, y_test\n",
    "\n",
    "def predict_test(\n",
    "                model: torch.nn.Module, \n",
    "                dataloader: torch.utils.data.DataLoader,\n",
    "                device: torch.device,\n",
    "                test_dir: str, \n",
    "                test_labels_dir: str,\n",
    "                batch_size: int,\n",
    "                y_test\n",
    "                ):\n",
    "    X_test_pred=[]\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            input = data.to(device)\n",
    "            output = (model.forward_once(input)).cpu()\n",
    "            output = np.array(output)\n",
    "            X_test_pred.append(output)\n",
    "    X_test_pred = np.concatenate(X_test_pred, axis = 0)\n",
    "    return X_test_pred\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/prediction.py\n",
    "\n",
    "import torch\n",
    "import prediction_engine\n",
    "import siamese_net\n",
    "import numpy as np\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "INPUT_SIZE = 2500\n",
    "OUTPUT_SIZE = 12\n",
    "CHANNELS=50\n",
    "KERNEL_SIZES=[50, 10]\n",
    "STRIDES=[2, 2]\n",
    "PADDINGS=[1, 1]\n",
    "HIDDEN_SIZES=[256]\n",
    "\n",
    "# Setup directories\n",
    "#test_dir = \"datasets/contest_TEST.h5\"\n",
    "test_labels_dir = \"datasets/test_labels.csv\"\n",
    "model_dir = 'models/final_model2_dashing_dream_256b_50ep.pth'\n",
    "test_dir = \"datasets/contest_TRAIN.h5\"\n",
    "\n",
    "\n",
    "# Setup target device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create DataLoaders with help from data_setup.py\n",
    "test_dataloader, y_test = prediction_engine.create_dataloaders(\n",
    "    test_dir=test_dir,\n",
    "    test_labels_dir=test_labels_dir,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device = device,\n",
    "    pred_test = False # USE WITH CAUTION, turn to 'False' if you want to get embeddings of the training data\n",
    ")\n",
    "\n",
    "\n",
    "saved_state_dict = torch.load(model_dir, map_location=torch.device('cpu'))\n",
    "\n",
    "# Create a new instance of your model\n",
    "model = siamese_net.SiameseNetwork(\n",
    "    input_size=INPUT_SIZE, \n",
    "    output_size=OUTPUT_SIZE, \n",
    "    channels=CHANNELS, \n",
    "    kernel_sizes=KERNEL_SIZES, \n",
    "    strides=STRIDES, \n",
    "    paddings=PADDINGS, \n",
    "    hidden_sizes=HIDDEN_SIZES\n",
    ").to(device)\n",
    "# Load the saved state into the new model instance\n",
    "model.load_state_dict(saved_state_dict)\n",
    "\n",
    "#todo save this to a file\n",
    "prediction_X_test = prediction_engine.predict_test(\n",
    "                    model=model, \n",
    "                    dataloader=test_dataloader,\n",
    "                    device=device,\n",
    "                    test_dir=test_dir, \n",
    "                    test_labels_dir=test_labels_dir,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    y_test=y_test\n",
    "                    )\n",
    "\n",
    "\n",
    "np.save('datasets/prediction_X_train_dashing_dream.npy', prediction_X_test)        \n",
    "np.save('datasets/y_train.npy', y_test)          \n",
    "\n",
    "#https://colab.research.google.com/drive/15D5vAYkhbAs5-txhYTCb_Fp2jiCnHXVN#scrollTo=82F_qINOBbkL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test = np.load('datasets/prediction_X_test.npy')\n",
    "test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "376601425dfe99d085b94bbaa5bb028434fb406f20f17f98957b701f4150938c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
